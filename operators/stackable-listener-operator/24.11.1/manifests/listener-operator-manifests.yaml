---
apiVersion: v1
kind: ConfigMap
metadata:
  name: listener-operator-deployer-manifests
data:
  deploy.sh: |
    #!/bin/env bash
    set -euox pipefail

    echo 'Deploying listener-operator manifests'

    export LISTENER_OPERATOR_DEPLOYER_DEPLOYMENT_UID=$(kubectl get deployment listener-operator-deployer -o go-template='{{.metadata.uid}}')
    export LISTENER_OPERATOR_CLUSTERROLE_NAME=$(kubectl get clusterrole -l olm.owner=listener-operator.v"$OP_VERSION",olm.owner.kind=ClusterServiceVersion \
    -o name | head -n 1 | sed 's/^.*\///')
    export LISTENER_OPERATOR_CLUSTERROLE_UID=$(kubectl get clusterrole $LISTENER_OPERATOR_CLUSTERROLE_NAME -o go-template='{{.metadata.uid}}')

    #
    # Patch the daemonset manifest with any tolerations from the subscription.
    #
    SUBSCRIPTION_TOLERATIONS=$(kubectl get deployment listener-operator-deployer -o jsonpath='{.spec.template.spec.tolerations}')
    export SUBSCRIPTION_TOLERATIONS="$SUBSCRIPTION_TOLERATIONS"

    for file in /manifests/*.yaml; do
      echo Deploying $file
      cat $file | envsubst | ./kubectl apply --wait -f -
    done

    echo 'Successfully deployed listener-operator manifests'

    sleep infinity

  csidriver.yaml: |
    ---
    apiVersion: storage.k8s.io/v1
    kind: CSIDriver
    metadata:
      name: listeners.stackable.tech
      ownerReferences:
        # We have to use a clusterScope object as an owner of another clusterScope object
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${LISTENER_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${LISTENER_OPERATOR_CLUSTERROLE_UID}"
    spec:
      attachRequired: false
      podInfoOnMount: true
      fsGroupPolicy: File
      volumeLifecycleModes:
        - Ephemeral
        - Persistent

  storageclass.yaml: |
    ---
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: listeners.stackable.tech
      ownerReferences:
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${LISTENER_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${LISTENER_OPERATOR_CLUSTERROLE_UID}"
    provisioner: listeners.stackable.tech
    volumeBindingMode: WaitForFirstConsumer

  node-daemonset.yaml: |
    ---
    # Source: listener-operator/templates/node-daemonset.yaml
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: listener-operator-node-daemonset
      labels:
        app.kubernetes.io/name: listener-operator
        app.kubernetes.io/instance: listener-operator
        stackable.tech/vendor: Stackable
        app.kubernetes.io/version: "24.11.1"
      ownerReferences:
        - apiVersion: apps/v1
          kind: Deployment
          name: listener-operator-deployer
          uid: "${LISTENER_OPERATOR_DEPLOYER_DEPLOYMENT_UID}"
    spec:
      selector:
        matchLabels:
          app.kubernetes.io/role: node
          app.kubernetes.io/name: listener-operator
          app.kubernetes.io/instance: listener-operator
          stackable.tech/vendor: Stackable
      template:
        metadata:
          labels:
            app.kubernetes.io/role: node
            app.kubernetes.io/name: listener-operator
            app.kubernetes.io/instance: listener-operator
            stackable.tech/vendor: Stackable
        spec:
          serviceAccountName: listener-operator-serviceaccount
          securityContext: {}
          containers:
            - name: listener-operator
              securityContext:
                runAsUser: 0
                seLinuxOptions:
                  type: spc_t
              image: "quay.io/stackable/listener-operator@sha256:b9c7957e7e562889bddd08e679abff05bb677672b3f735932f68514b77eacfd5"
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
              args:
                - run
                - node
              env:
                - name: CSI_ENDPOINT
                  value: /csi/csi.sock
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.nodeName
              volumeMounts:
                - name: csi
                  mountPath: /csi
                - name: mountpoint
                  mountPath: /var/lib/kubelet/pods
            - name: node-driver-registrar
              image: "quay.io/stackable/sig-storage/csi-node-driver-registrar@sha256:161e754437428c68490333337303fb30b695a9ac677c3a00a1dc27565b855ab8"
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
              args:
                - --csi-address=/csi/csi.sock
                - --kubelet-registration-path=/var/lib/kubelet/plugins/listeners.stackable.tech/csi.sock
              volumeMounts:
                - name: registration-sock
                  mountPath: /registration
                - name: csi
                  mountPath: /csi
          volumes:
            - name: registration-sock
              hostPath:
                # node-driver-registrar appends a driver-unique filename to this path to avoid conflicts
                # see https://github.com/stackabletech/secret-operator/issues/229 for why this path should not be too long
                path: /var/lib/kubelet/plugins_registry
            - name: csi
              hostPath:
                path: /var/lib/kubelet/plugins/listeners.stackable.tech/
            - name: mountpoint
              hostPath:
                path: /var/lib/kubelet/pods/
          tolerations: $SUBSCRIPTION_TOLERATIONS

  cluster-internal.yaml: |
    ---
    apiVersion: listeners.stackable.tech/v1alpha1
    kind: ListenerClass
    metadata:
      name: cluster-internal
      ownerReferences:
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${LISTENER_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${LISTENER_OPERATOR_CLUSTERROLE_UID}"
    spec:
      serviceType: ClusterIP

  external-stable.yaml: |
    ---
    # Source: listener-operator/templates/listener-classes.yaml
    apiVersion: listeners.stackable.tech/v1alpha1
    kind: ListenerClass
    metadata:
      name: external-stable
      ownerReferences:
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${LISTENER_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${LISTENER_OPERATOR_CLUSTERROLE_UID}"
    spec:
      # To support environments that don't have LoadBalancer support, we fall back to using NodePort Services for "stable addresses".
      # To ensure that these addresses are actually stable, the Listener Operator will "bind" such Listeners to only run on the Node that
      # they were originally scheduled to (as controlled by the lifetime of the PersistentVolume). The downside here is that we prevent Kubernetes
      # from migrating that workload to a different Node (such as if it goes down).
      #
      # This is useful for cloud environments with no Cloud Controller Manager (or one which doesn't support LoadBalancers),
      # or on-premise environments that don't support external LoadBalancer peering (such as Calico (https://docs.tigera.io/calico/latest/networking/configuring/advertise-service-ips)
      # or MetalLB (https://metallb.org/)).
      serviceType: NodePort

  external-unstable.yaml: |
    ---
    # Source: listener-operator/templates/listener-classes.yaml
    apiVersion: listeners.stackable.tech/v1alpha1
    kind: ListenerClass
    metadata:
      name: external-unstable
      ownerReferences:
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${LISTENER_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${LISTENER_OPERATOR_CLUSTERROLE_UID}"
    spec:
      serviceType: NodePort

  listener-scc.yaml: |
    ---
    apiVersion: security.openshift.io/v1
    kind: SecurityContextConstraints
    metadata:
      name: listener-scc
      labels:
        stackable.tech/vendor: Stackable
      annotations:
        kubernetes.io/description:
          listener-scc includes the minimum required privileges to run the listener-operator.
    allowHostDirVolumePlugin: true
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowHostPorts: false
    allowPrivilegeEscalation: false
    allowPrivilegedContainer: false
    allowedCapabilities: []
    defaultAddCapabilities: null
    fsGroup:
      type: MustRunAs
    groups: []
    priority: null
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
      - ALL
    runAsUser:
      type: MustRunAs
      uid: 0
    seLinuxContext:
      # Permit access to the resources in all namespaces
      type: RunAsAny
    seccompProfiles:
      - runtime/default
    supplementalGroups:
      type: RunAsAny
    users: []
    volumes:
      - emptyDir
      - hostPath
      - projected

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: secret-operator-deployer-manifests
data:
  deploy.sh: |
    #!/bin/env bash

    set -euox pipefail

    echo 'Deploying secret-operator manifests'

    SECRET_OPERATOR_DEPLOYER_DEPLOYMENT_UID=$(kubectl get deployment secret-operator-deployer -n "$NAMESPACE" -o go-template='{{.metadata.uid}}')
    export SECRET_OPERATOR_DEPLOYER_DEPLOYMENT_UID="$SECRET_OPERATOR_DEPLOYER_DEPLOYMENT_UID"

    SECRET_OPERATOR_CLUSTERROLE_NAME=$(kubectl get clusterrole -l olm.owner=secret-operator.v"$OP_VERSION",olm.owner.kind=ClusterServiceVersion \
    -o name | head -n 1 | sed 's/^.*\///')
    export SECRET_OPERATOR_CLUSTERROLE_NAME="$SECRET_OPERATOR_CLUSTERROLE_NAME"

    SECRET_OPERATOR_CLUSTERROLE_UID=$(kubectl get clusterrole "$SECRET_OPERATOR_CLUSTERROLE_NAME" -o go-template='{{.metadata.uid}}')
    export SECRET_OPERATOR_CLUSTERROLE_UID="$SECRET_OPERATOR_CLUSTERROLE_UID"

    for file in /manifests/*.yaml; do
      echo Deploying $file
      cat $file | envsubst | ./kubectl apply --wait -f -
    done

    echo 'Successfully deployed secret-operator manifests'

    sleep infinity
  stackable-secret-operator-scc.yaml: |
    apiVersion: security.openshift.io/v1
    kind: SecurityContextConstraints
    metadata:
      name: stackable-secret-operator-scc
      annotations:
        kubernetes.io/description: SCC for Stackable secret operator
    allowHostDirVolumePlugin: true
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowHostPorts: false
    allowPrivilegeEscalation: true
    allowPrivilegedContainer: true # Needed because otherwise we get spec.template.spec.containers[0].volumeMounts.mountPropagation: Forbidden: Bidirectional mount propagation is available only to privileged containers
    allowedCapabilities: null
    defaultAddCapabilities: null
    fsGroup:
      type: RunAsAny
    groups: []
    priority: null
    readOnlyRootFilesystem: false
    requiredDropCapabilities:
    - MKNOD
    runAsUser:
      type: RunAsAny
    seLinuxContext:
      type: MustRunAs
    supplementalGroups:
      type: RunAsAny
    users: []
    volumes:
    - downwardAPI
    - projected
    - hostPath
    - emptyDir
  tls.yaml: |
    ---
    apiVersion: secrets.stackable.tech/v1alpha1
    kind: SecretClass
    metadata:
      name: tls
      ownerReferences:
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${SECRET_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${SECRET_OPERATOR_CLUSTERROLE_UID}"
    spec:
      backend:
        autoTls:
          ca:
            secret:
              name: secret-provisioner-tls-ca
              namespace: "${NAMESPACE}"
            autoGenerate: true
  csidriver.yaml: |
    apiVersion: storage.k8s.io/v1
    kind: CSIDriver
    metadata:
      name: secrets.stackable.tech
      ownerReferences:
        # We have to use a clusterScope object as an owner of another clusterScope object
        # Strangely OpenShift will not delete the `secret-operator-clusterrole` ClusterRole that we *officially* rolled out using the standard mechanism
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${SECRET_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${SECRET_OPERATOR_CLUSTERROLE_UID}"
    spec:
      attachRequired: false
      podInfoOnMount: true
      fsGroupPolicy: File
      volumeLifecycleModes:
        - Ephemeral
        - Persistent
  storageclass.yaml: |
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: secrets.stackable.tech
      ownerReferences:
        - apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          name: "${SECRET_OPERATOR_CLUSTERROLE_NAME}"
          uid: "${SECRET_OPERATOR_CLUSTERROLE_UID}"
    provisioner: secrets.stackable.tech
  daemonset.yaml: |
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: secret-operator-daemonset
      labels:
        app.kubernetes.io/instance: secret-operator
        app.kubernetes.io/name: secret-operator
        app.kubernetes.io/version: 24.7.0
      # We have to use a namespaced object as an owner of another namespaced object
      ownerReferences:
        - apiVersion: apps/v1
          kind: Deployment
          name: secret-operator-deployer
          uid: "${SECRET_OPERATOR_DEPLOYER_DEPLOYMENT_UID}"
    spec:
      selector:
        matchLabels:
          app.kubernetes.io/name: secret-operator
          app.kubernetes.io/instance: secret-operator
      template:
        metadata:
          labels:
            app.kubernetes.io/name: secret-operator
            app.kubernetes.io/instance: secret-operator
        spec:
          serviceAccountName: secret-operator-serviceaccount
          securityContext: {}
          containers:
            - name: secret-operator
              securityContext:
                privileged: true
                runAsUser: 0
              image: "quay.io/stackable/secret-operator@sha256:97a072a77fa53f52d2b0215fe40c14aa4ef058b14f7637e75402177d7cc9a464"
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
              env:
                - name: CSI_ENDPOINT
                  value: /csi/csi.sock
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.nodeName
                - name: PRIVILEGED
                  value: "true"
              volumeMounts:
                - name: csi
                  mountPath: /csi
                - name: mountpoint
                  mountPath: /var/lib/kubelet/pods
                  mountPropagation: Bidirectional
                - name: tmp
                  mountPath: /tmp
            - name: external-provisioner
              image: "quay.io/stackable/sig-storage/csi-provisioner@sha256:7de2c30f90aa30931e508a179561d4ec6a533d08da4bee9b019bf189c8f30732"
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
              args:
                - --csi-address=/csi/csi.sock
                - --feature-gates=Topology=true
                - --extra-create-metadata
              volumeMounts:
                - name: csi
                  mountPath: /csi
            - name: node-driver-registrar
              image: "quay.io/stackable/sig-storage/csi-node-driver-registrar@sha256:cb4453ceb2166f29d0a4299b58035f0d553b3d35a6b40f1ac767d249a2be2217"
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
              args:
                - --csi-address=/csi/csi.sock
                - --kubelet-registration-path=/var/lib/kubelet/plugins/secrets.stackable.tech/csi.sock
              volumeMounts:
                - name: registration-sock
                  mountPath: /registration
                - name: csi
                  mountPath: /csi
          volumes:
            - name: registration-sock
              hostPath:
                # node-driver-registrar appends a driver-unique filename to this path to avoid conflicts
                # see https://github.com/stackabletech/secret-operator/issues/229 for why this path should not be too long
                path: /var/lib/kubelet/plugins_registry
            - name: csi
              hostPath:
                path: /var/lib/kubelet/plugins/secrets.stackable.tech/
            - name: mountpoint
              hostPath:
                path: /var/lib/kubelet/pods/
            - name: tmp
              emptyDir: {}
